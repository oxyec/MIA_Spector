{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.60s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from attack_text import *\n",
    "from utils.metrics_utils import Metrics\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm_path_7b = \"/home/wanghuili/MIA-Spector/models/llama/llama-13b-hf\"\n",
    "tokenizer_7b = AutoTokenizer.from_pretrained(llm_path_7b, trust_remote_code=True)\n",
    "model_7b = AutoModelForCausalLM.from_pretrained(llm_path_7b, device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = \"WikiMIA_length32\"\n",
    "OUT_JSON = f\"llama_data/threshold_{DATA_NAME}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring Samples: 100%|██████████| 100/100 [10:32<00:00,  6.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# ======= 配置 =======\n",
    "MAX_SAMPLES = 100\n",
    "MINKPP_RATIO = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "ALPHA = 0.01                          # 目标 FPR（用于 threshold_fpr_alpha）\n",
    "\n",
    "# 键名工具：规避浮点字符串不一致\n",
    "def rstr(r: float) -> str:\n",
    "    return f\"{r:.1f}\"\n",
    "\n",
    "# 组合出所有要统计的键\n",
    "keys_minkpp   = [f\"mink++_{rstr(r)}\" for r in MINKPP_RATIO]\n",
    "keys_mink     = [f\"mink_{rstr(r)}\"   for r in MINKPP_RATIO]\n",
    "\n",
    "scores_perplexity_1 = [f\"perplexity_1_mink++_{rstr(r)}\" for r in MINKPP_RATIO]\n",
    "scores_perplexity_2 = [f\"perplexity_2_mink++_{rstr(r)}\" for r in MINKPP_RATIO]\n",
    "scores_perplexity_3 = [f\"perplexity_3_mink++_{rstr(r)}\" for r in MINKPP_RATIO]\n",
    "scores_perplexity_4 = [f\"perplexity_4_mink++_{rstr(r)}\" for r in MINKPP_RATIO]\n",
    "\n",
    "ppl_scores_label = [\n",
    "    \"perplexity_variance\", \"perplexity_std\", \"perplexity_range\",\n",
    "    \"perplexity_skewness\", \"perplexity_kurtosis\"\n",
    "]\n",
    "\n",
    "# ======= 初始化 =======\n",
    "cal = ScoreCalculator(model=model_7b, tokenizer=tokenizer_7b)\n",
    "data_wiki128 = DatasetLoader.load_dataset(f\"{DATA_NAME}\", MAX_SAMPLES)\n",
    "\n",
    "# 容器\n",
    "scores_by_ratio = {k: [] for k in (keys_minkpp + keys_mink)}\n",
    "scores_by_ppl = {k: [] for k in ppl_scores_label}\n",
    "scores_by_ppl_1 = {k: [] for k in scores_perplexity_1}\n",
    "scores_by_ppl_2 = {k: [] for k in scores_perplexity_2}\n",
    "scores_by_ppl_3 = {k: [] for k in scores_perplexity_3}\n",
    "scores_by_ppl_4 = {k: [] for k in scores_perplexity_4}\n",
    "labels = []\n",
    "\n",
    "def safe_get(d: dict, k: str, default=np.nan):\n",
    "    return d[k] if k in d else default\n",
    "\n",
    "# ======= 打分 =======\n",
    "for d in tqdm(data_wiki128, desc=\"Scoring Samples\"):\n",
    "    text = d[\"input\"]\n",
    "    label = int(d[\"label\"])\n",
    "    # 重要：把需要的 ratio 列表传进去\n",
    "    s, _ = cal.calculate_scores(text)\n",
    "\n",
    "    labels.append(label)\n",
    "\n",
    "    for k in keys_minkpp:\n",
    "        scores_by_ratio[k].append(safe_get(s, k))\n",
    "    for k in keys_mink:\n",
    "        scores_by_ratio[k].append(safe_get(s, k))\n",
    "    for k in ppl_scores_label:\n",
    "        scores_by_ppl[k].append(safe_get(s, k))\n",
    "    for k in scores_perplexity_1:\n",
    "        scores_by_ppl_1[k].append(safe_get(s, k))\n",
    "    for k in scores_perplexity_2:\n",
    "        scores_by_ppl_2[k].append(safe_get(s, k))\n",
    "    for k in scores_perplexity_3:\n",
    "        scores_by_ppl_3[k].append(safe_get(s, k))\n",
    "    for k in scores_perplexity_4:\n",
    "        scores_by_ppl_4[k].append(safe_get(s, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_DIRECTIONS = {\n",
    "    \"perplexity_variance\": -1,\n",
    "    \"perplexity_std\" : -1,\n",
    "    \"perplexity_range\" : -1,\n",
    "    \"perplexity_skewness\" : -1,\n",
    "    \"perplexity_kurtosis\" : -1\n",
    "}                                              \n",
    "\n",
    "labels = np.asarray(labels, dtype=int)\n",
    "labels_non = (labels == 0)\n",
    "\n",
    "# ======= 工具：按非成员分布计算 threshold_fpr_alpha =======\n",
    "def threshold_from_nonmember(non_scores, direction_sign, alpha):\n",
    "    arr = np.asarray(non_scores, dtype=float)\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    if direction_sign >= 0:\n",
    "        return float(np.quantile(arr, 1.0 - alpha))  # 越大越像成员 → 取 (1 - alpha) 分位\n",
    "    else:\n",
    "        return float(np.quantile(arr, alpha))  # 越小越像成员 → 取 alpha 分位\n",
    "\n",
    "# ======= 工具：指标方向（默认 mink/mink++ 是正向；ppl统计默认正向，可在 CUSTOM_DIRECTIONS 覆盖）=======\n",
    "def metric_direction(metric_name: str) -> int:\n",
    "    if metric_name in CUSTOM_DIRECTIONS:\n",
    "        return CUSTOM_DIRECTIONS[metric_name]\n",
    "    low = metric_name.lower()\n",
    "    if low.startswith(\"mink\") or low.startswith(\"mink++\") or low.startswith(\"perplexity_\"):\n",
    "        return +1\n",
    "    return +1\n",
    "\n",
    "config = {\n",
    "    \"data_name\": DATA_NAME,\n",
    "    \"alpha\": ALPHA,\n",
    "    \"counts\": {\n",
    "        \"total\": int(len(labels)),\n",
    "        \"member\": int((labels == 1).sum()),\n",
    "        \"non_member\": int((labels == 0).sum())\n",
    "    },\n",
    "    \"items\": []\n",
    "}\n",
    "\n",
    "from typing import Dict, Any, List, Optional\n",
    "import numpy as np\n",
    "\n",
    "# --- ECDF: 控体积，便于上线 ---\n",
    "def build_ecdf(values: np.ndarray, max_points: int = 2048) -> Dict[str, List[float]]:\n",
    "    x = np.sort(values.astype(float))\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return {\"xs\": [], \"cdf\": []}\n",
    "    if n > max_points:\n",
    "        idx = np.linspace(0, n - 1, num=max_points).astype(int)\n",
    "        x = x[idx]; n = len(x)\n",
    "    cdf = (np.arange(1, n+1, dtype=float) / n)\n",
    "    return {\"xs\": x.tolist(), \"cdf\": cdf.tolist()}\n",
    "\n",
    "# --- Platt 标定（用 sklearn；若无可选跳过） ---\n",
    "def fit_platt(scores: np.ndarray, labels: np.ndarray) -> Optional[Dict[str, float]]:\n",
    "    try:\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "    except Exception:\n",
    "        return None\n",
    "    X = scores.reshape(-1, 1).astype(float)\n",
    "    y = labels.astype(int)\n",
    "    if len(np.unique(y)) < 2:\n",
    "        return None\n",
    "    lr = LogisticRegression(solver=\"lbfgs\")\n",
    "    lr.fit(X, y)\n",
    "    a = float(lr.coef_[0, 0]); b = float(lr.intercept_[0])\n",
    "    return {\"a\": a, \"b\": b}\n",
    "\n",
    "# --- 非成员分布阈值（注意：这里假定传入的是“已统一为越大越像成员”的分数） ---\n",
    "def threshold_from_nonmember_posdir(non_scores_posdir: np.ndarray, alpha: float) -> float:\n",
    "    arr = np.asarray(non_scores_posdir, dtype=float)\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    # 越大越像成员 → FPR=alpha 时阈值取 (1 - alpha) 分位\n",
    "    return float(np.quantile(arr, 1.0 - alpha))\n",
    "\n",
    "def pack_metric(name: str, scores_list: list):\n",
    "    # 原始分数（未反转）\n",
    "    scores_raw = np.asarray(scores_list, dtype=float)\n",
    "    mask = np.isfinite(scores_raw)\n",
    "    scores_raw = scores_raw[mask]\n",
    "    labs = labels[mask]\n",
    "    if np.unique(labs).size < 2 or scores_raw.size == 0:\n",
    "        return None\n",
    "\n",
    "    # 方向与反转：在“正向空间”（越大越像成员）里做所有统计/阈值/AUC\n",
    "    dire = metric_direction(name)   # +1 / -1\n",
    "    scores_posdir = scores_raw if dire == +1 else -scores_raw\n",
    "\n",
    "    # === 1) 曲线与阈值（用你现有的 Metrics；传入“正向分数”） ===\n",
    "    out = Metrics.calculate_metrics(scores_posdir, labs)  # 你的实现不需要 larger_is_positive 参数\n",
    "    if len(out) == 6:\n",
    "        auroc, fpr95, tpr05, fpr_list, tpr_list, best_thresh_posdir = out\n",
    "        best_j = None\n",
    "    elif len(out) >= 7:\n",
    "        auroc, fpr95, tpr05, fpr_list, tpr_list, best_thresh_posdir, best_j = out[:7]\n",
    "    else:\n",
    "        auroc, fpr95, tpr05 = out[:3]\n",
    "        best_thresh_posdir, best_j = (np.nan, None)\n",
    "\n",
    "    # 把 bestJ 阈值“还原回原始方向”的数值（便于人读/一致性存储）\n",
    "    best_thresh_raw = best_thresh_posdir if dire == +1 else -best_thresh_posdir\n",
    "\n",
    "    # === 2) 非成员分布阈值 @ FPR=alpha（在正向空间里取 (1-α) 分位，再还原） ===\n",
    "    non_scores_posdir = scores_posdir[labs == 0]\n",
    "    tau_alpha_posdir = threshold_from_nonmember_posdir(non_scores_posdir, ALPHA)\n",
    "    tau_alpha_raw = tau_alpha_posdir if dire == +1 else -tau_alpha_posdir\n",
    "\n",
    "    # === 3) 统计量（两类分布，正向空间） ===\n",
    "    memb = scores_posdir[labs == 1]\n",
    "    nonm = non_scores_posdir\n",
    "    stats = {\n",
    "        \"posdir_means\":   {\"member\": float(np.mean(memb)) if memb.size else np.nan,\n",
    "                           \"non_member\": float(np.mean(nonm)) if nonm.size else np.nan},\n",
    "        \"posdir_stds\":    {\"member\": float(np.std(memb))  if memb.size else np.nan,\n",
    "                           \"non_member\": float(np.std(nonm))  if nonm.size else np.nan},\n",
    "        \"posdir_quantiles\": {\n",
    "            \"member\":     {q: float(np.quantile(memb, q)) for q in [0.01,0.05,0.5,0.95,0.99]} if memb.size else {},\n",
    "            \"non_member\": {q: float(np.quantile(nonm, q)) for q in [0.01,0.05,0.5,0.95,0.99]} if nonm.size else {}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # === 4) ECDF（正向空间；用于线上 p-value / 置信度） ===\n",
    "    ecdf = {\n",
    "        \"non_member_posdir_ecdf\": build_ecdf(nonm),\n",
    "        # 可选：也存 member 的，便于似然比/贝叶斯后验\n",
    "        \"member_posdir_ecdf\": build_ecdf(memb) if memb.size else {\"xs\": [], \"cdf\": []}\n",
    "    }\n",
    "\n",
    "    # === 5) Platt 标定（正向空间；p = σ(a*s_posdir + b)） ===\n",
    "    calib = fit_platt(scores_posdir, labs)  # 可能返回 None\n",
    "\n",
    "    # === 6) 汇总（阈值以“原始方向”存储；同时保留方向符号供线上做比较） ===\n",
    "    return {\n",
    "        \"metric\": name,\n",
    "        \"direction\": \"+\" if dire >= 0 else \"-\",\n",
    "        \"AUROC\": float(auroc),\n",
    "        \"FPR@TPR=95%\": float(fpr95),\n",
    "        \"TPR@FPR=5%\": float(tpr05),\n",
    "        \"threshold_bestJ\": float(best_thresh_raw) if best_thresh_posdir is not None else None,\n",
    "        \"threshold_fpr_alpha\": float(tau_alpha_raw) if tau_alpha_posdir is not None else None,\n",
    "        \"Youden_J\": (float(best_j) if best_j is not None else None),\n",
    "        \"stats\": stats,\n",
    "        \"ecdf\": ecdf,\n",
    "        \"calibrator\": calib  # 形如 {\"a\":..., \"b\":...} 或 None\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存常数，config get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ threshold config saved to llama_data/threshold_WikiMIA_length32.json\n"
     ]
    }
   ],
   "source": [
    "# 1) mink++ / mink\n",
    "for ratio in MINKPP_RATIO:\n",
    "    r = rstr(ratio)\n",
    "    cfg1 = pack_metric(f\"mink++_{r}\", scores_by_ratio[f\"mink++_{r}\"])\n",
    "    cfg2 = pack_metric(f\"mink_{r}\",   scores_by_ratio[f\"mink_{r}\"])\n",
    "    if cfg1: config[\"items\"].append(cfg1)\n",
    "    if cfg2: config[\"items\"].append(cfg2)\n",
    "\n",
    "# 2) ppl × mink++\n",
    "for ratio in MINKPP_RATIO:\n",
    "    r = rstr(ratio)\n",
    "    for i, bag in enumerate([scores_by_ppl_1, scores_by_ppl_2, scores_by_ppl_3, scores_by_ppl_4], start=1):\n",
    "        name = f\"perplexity_{i}_mink++_{r}\"\n",
    "        cfg = pack_metric(name, bag.get(name, []))\n",
    "        if cfg: config[\"items\"].append(cfg)\n",
    "\n",
    "# 3) Other Targets\n",
    "for name in ppl_scores_label:\n",
    "    cfg = pack_metric(name, scores_by_ppl[name])\n",
    "    if cfg: config[\"items\"].append(cfg)\n",
    "\n",
    "import json\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ threshold config saved to {OUT_JSON}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
